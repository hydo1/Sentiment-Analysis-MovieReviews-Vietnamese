import streamlit as st
import pandas as pd
import re
import unicodedata
import emoji
import requests
import joblib
import matplotlib.pyplot as plt
from wordcloud import WordCloud

from pyvi import ViTokenizer
from collections import Counter

# ======================
# LOAD MODEL
# ======================
tfidf = joblib.load("tfidf.pkl")
model = joblib.load("sentiment_model.pkl")

# ======================
# PREPROCESS
# ======================

SLANG_MAP = {
    "ko": "kh√¥ng",
    "k": "kh√¥ng",
    "kg": "kh√¥ng",
    "hok": "kh√¥ng",
    "hk": "kh√¥ng",
    "khum": "kh√¥ng",
    "kh": "kh√¥ng",
    "hem": "kh√¥ng",
    "kh√¥g": "kh√¥ng",
    "khog": "kh√¥ng",


    "10ƒë": "10 ƒëi·ªÉm",
    "ƒë·ªâm": "ƒëi·ªÉm",
    "dvien": "di·ªÖn vi√™n",
    "dv": "di·ªÖn vi√™n",
    "rv": "review",
    "ng": "ng∆∞·ªùi",
    "ch·ªõt": "ch·∫øt",
    "c·ª´": "c∆∞·ªùi",
    "d·ª°": "d·ªü",
    "rcm": "recommend",
    "nv": "nh√¢n v·∫≠t",
    "nvat": "nh√¢n v·∫≠t",
    "nd": "n·ªôi dung",
    "thi·ªác": "thi·ªát",
    "nh√£m": "nh·∫£m",
    "ƒëv": "ƒë·ªëi v·ªõi",
    "lun": "lu√¥n",
    "m√∫n": "mu·ªën",

    
    "c≈©m": "c≈©ng",
    "cx": "c≈©ng",
    "vs": "v·ªõi",
    "nma": "nh∆∞ng m√†",
    "bth": "b√¨nh th∆∞·ªùng",
    "zui": "vui",
    "bngu": "bu·ªìn ng·ªß",
    "tr": "tr·ªùi",
    "lquan": "li√™n quan",
    "tg": "th·ªùi gian",    

    "dc": "ƒë∆∞·ª£c",
    "ƒëc": "ƒë∆∞·ª£c",
    "dk": "ƒë∆∞·ª£c",


    "oke": "ok",
    "okie": "ok",

    "vl": "v√£i",
    "vcl": "v√£i",
    "v": "v·∫≠y",


    "t": "t√¥i",       
    "mk": "m√¨nh",
    "mn": "m·ªçi ng∆∞·ªùi",
    "mng": "m·ªçi ng∆∞·ªùi",
    "c·ªßng": "c≈©ng",
    "c≈©g": "c≈©ng",

}

STOPWORDS = {
    "v√†","v·ªõi","th√¨","l√†","m√†","nh√©","nh·ªâ","ch·ª©","ƒë·∫•y","n√†y","·∫•y","ƒë√≥","th·∫ø","v·∫≠y",
    "∆°","·ªù","·ª´","√†","ƒë√£","ƒëang","r·∫±ng","trong","khi","n√†o","ƒë·ªÉ","t·∫°i","t·ª´ng",
    "ra","v√†o","∆°i"
}

BLOCKED_WORDS = {"pass"}

# =====================================================
# 1. CLEAN + NORMALIZE 
# =====================================================
def clean_text(text):
    if not isinstance(text, str):
        return ""

    text = unicodedata.normalize("NFC", text).lower()

    # remove url
    text = re.sub(r"http\S+|www\.\S+", "", text)

    # slang
    for k, v in SLANG_MAP.items():
        text = re.sub(rf"\b{k}\b", v, text)

    # newline + k√©o d√†i
    text = text.replace("\n", " ").replace("\r", " ")
    text = re.sub(r"(.)\1{2,}", r"\1", text)

    # t√°ch emoji
    text = "".join(
        f" {ch} " if ch in emoji.EMOJI_DATA else ch
        for ch in text
    )

    # b·ªè d·∫•u c√¢u
    text = re.sub(r"[.,!?‚Ä¶;:()\"']", " ", text)

    # normalize space
    text = re.sub(r"\s+", " ", text).strip()

    words = [w for w in text.split() if w not in STOPWORDS]

    return " ".join(words)


# ==========================================


API_KEY = ""            # <--- THAY = API RI√äNG
VIDEO_ID = "h7RF-PBu-YM"          # <- ID video, vd: "dQw4w9WgXcQ"

import re

def extract_video_id(url):
    # case 1: youtube.com/watch?v=ID
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    
    # case 2: youtu.be/ID
    m = re.search(r"youtu\.be/([^?&]+)", url)
    if m:
        return m.group(1)
    
    return None

# ==========================================
# GET VIDEO INFO
# ==========================================
def get_video_info(video_id, api_key):
    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {
        "part": "snippet,statistics",
        "id": video_id,
        "key": api_key
    }

    data = requests.get(url, params=params).json()

    if not data["items"]:
        print("Kh√¥ng t√¨m th·∫•y video.")
        return None

    info = data["items"][0]
    snippet = info["snippet"]
    stats = info["statistics"]

    return {
        "video_id": video_id,
        "video_title": snippet.get("title"),
        "video_url": f"https://www.youtube.com/watch?v={video_id}",
        "channel_title": snippet.get("channelTitle"),
        "publishedAt": snippet.get("publishedAt"),
        "video_likeCount": stats.get("likeCount"),
        "video_commentCount": stats.get("commentCount")
    }


# ==========================================
# GET COMMENTS
# ==========================================
def get_youtube_comments(video_id, api_key, max_comments=4000, max_pages=1000):
    comments = []
    base_url = "https://www.googleapis.com/youtube/v3/commentThreads"

    params = {
        "part": "snippet",
        "videoId": video_id,
        "key": api_key,
        "textFormat": "plainText",
        "maxResults": 100
    }

    page_count = 0

    while True:
        response = requests.get(base_url, params=params).json()

        if "items" not in response:
            print("Error:", response)
            break

        for item in response["items"]:
            c = item["snippet"]["topLevelComment"]["snippet"]
            comments.append({
                "author": c.get("authorDisplayName"),
                "text": c.get("textDisplay"),
                "likeCount": c.get("likeCount"),
                "publishedAt": c.get("publishedAt")
            })

            # D·ª™NG NGAY KHI ƒê·ª¶ 300 COMMENT
            if len(comments) >= max_comments:
                return comments

        page_count += 1
        print(f"Fetched page {page_count}, total comments: {len(comments)}")

        if "nextPageToken" in response and page_count < max_pages:
            params["pageToken"] = response["nextPageToken"]
        else:
            break

    return comments


# ==========================================
# RUN
# ==========================================

print("Fetching video info...")
video_info = get_video_info(VIDEO_ID, API_KEY)

print("Fetching comments...")
comments = get_youtube_comments(VIDEO_ID, API_KEY)

# G·ªôp th√™m th√¥ng tin video v√†o t·ª´ng comment
for c in comments:
    c.update(video_info)

# T·∫°o DataFrame
df = pd.DataFrame(comments)

# L∆∞u CSV
#df.to_csv("youtube_comments_full.csv", index=False, encoding="utf-8-sig")

# ======================
# PREDICT
# ======================
def predict_sentiment(comments):
    df = pd.DataFrame({"comment": comments})

    df["cleaned_text"] = df["comment"].apply(clean_text)
    df = df[~df["cleaned_text"].str.contains("|".join(BLOCKED_WORDS))] #filter

    #df["length"] = df["cleaned_text"].apply(lambda x: len(x.split()))
    #df["has_emoji"] = df["comment"].apply(has_emoji_fn)

    X_text = tfidf.transform(df["cleaned_text"])
    df["sentiment"] = model.predict(X_text)

    return df

# ======================
# STREAMLIT UI
# ======================
st.set_page_config(page_title="YouTube Sentiment Analysis")

st.title("üé¨ YouTube Movie Review Sentiment")

url = st.text_input("Nh·∫≠p link YouTube")

if st.button("Ph√¢n t√≠ch"):
    video_id = extract_video_id(url)

    if video_id is None:
        st.error("‚ùå Link YouTube kh√¥ng h·ª£p l·ªá")
    else:
        with st.spinner("ƒêang crawl & ph√¢n t√≠ch..."):
            # 1. L·∫•y info video
            video_info = get_video_info(video_id, API_KEY)

            if video_info is None:
                st.error("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c th√¥ng tin video")
                st.stop()

            # 2. Hi·ªÉn th·ªã TITLE video
            thumbnail_url = f"https://img.youtube.com/vi/{video_id}/hqdefault.jpg"
            st.image(
            thumbnail_url,
            caption="Thumbnail video",
            use_container_width=True
            )
            st.subheader("üé• Th√¥ng tin video")
            st.markdown(f"**üìå Ti√™u ƒë·ªÅ:** {video_info['video_title']}")
            st.markdown(f"**üì∫ K√™nh:** {video_info['channel_title']}")
            st.markdown(f"**üëç Like:** {video_info['video_likeCount']}")
            st.markdown(f"**üí¨ Comment:** {video_info['video_commentCount']}")

            # 3. L·∫•y comment
            comments_raw = get_youtube_comments(video_id, API_KEY)

            comments_text = [c["text"] for c in comments_raw]

            # 4. Predict sentiment
            df_result = predict_sentiment(comments_text)

        st.success("‚úÖ Ph√¢n t√≠ch xong!")

        # ======================
        # VISUALIZE
        # ======================
        def plot_wordcloud_by_label(df, label):
            text = " ".join(df[df["sentiment"] == label]["cleaned_text"])

            if text.strip() == "":
                st.info(f"Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho nh√£n {label}")
                return

            wc = WordCloud(
                width=800,
                height=400,
                background_color="white",
                collocations=False
            )

            fig, ax = plt.subplots(figsize=(10,5))
            ax.imshow(wc.generate(text))
            ax.axis("off")
            ax.set_title(f"WordCloud ‚Äì {label}")

            st.pyplot(fig)

        def plot_sentiment_pie(df):
            counts = df["sentiment"].value_counts()

            fig, ax = plt.subplots()
            ax.pie(
                counts.values,
                labels=counts.index,
                autopct="%1.1f%%",
                startangle=90
            )
            ax.axis("equal")
            ax.set_title("T·ª∑ l·ªá c·∫£m x√∫c b√¨nh lu·∫≠n")

            st.pyplot(fig)
        
        st.subheader("üìä Ph√¢n b·ªë c·∫£m x√∫c")
        st.bar_chart(df_result["sentiment"].value_counts())

        st.subheader("üìä T·ª∑ l·ªá c·∫£m x√∫c (Pie chart)")
        plot_sentiment_pie(df_result)

        st.subheader("‚òÅÔ∏è WordCloud theo nh√£n c·∫£m x√∫c")
        labels = ["positive", "mixed/neutral", "negative"]
        for lb in labels:
            plot_wordcloud_by_label(df_result, lb)

        st.subheader("üí¨ K·∫øt qu·∫£ chi ti·∫øt")
        st.dataframe(df_result[["comment", "sentiment"]])